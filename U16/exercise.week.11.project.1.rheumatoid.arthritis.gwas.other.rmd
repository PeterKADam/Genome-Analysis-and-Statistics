---
title: "Project A (3)"
author: "Palle Villesen"
output: 
  html_document:
    code_folding: show
    theme: cosmo
    toc: yes
editor_options: 
  chunk_output_type: console
---

# Learning goals for this exercise

What you will do:

 * Run 280.000 tests
 * Use R's fishers exact test to get a p value 
 * Adjusting for multiple testing
 * Compare all the results
 
# R functions used this week

 * basic math like +,-,/,*
 * for() loops
 * chisq.test()
 * fisher.test()
 * p.adjust()   # Adjust p values
 * select()     # select variables from a tibble
 * everything() # a helper function that select all remaining variables
 * ls()
 * rm()

# Reading project data

```{r}
library(tidyverse)

snpdata <- read_rds(r"(U16\gwas_allele_counts.julia.2008.rds)")

# snpdata
```

## Testing all 280.000 SNPs

We are using fisher.test() to get exact p values.

You should have a look at variables x and y before and after the loop.

The loop will output a status every 5000 SNPs - so you can estimate how long it will take.

(On my laptop it's about 40.000 SNPs pr minute running fisher.test () and about 280.000 pr minute for chisq.test())

```{r}
x1 <- rep(NA, nrow(snpdata))
y <- snpdata$contingency_table

for (i in 1:length(x1)) {
    result <- fisher.test(y[[i]])
    x1[i] <- result$p.value
    if (i %% 5000 == 0) {
        cat(as.character(Sys.time()), i, "\n")
    } # Print status every 5000 SNPs
}

snpdata$p.value <- x1

rm(x1, y, result)
```


## Inspect results

```{r}

snpdata %>%
    select(p.value, everything())

```

## Save the data for future use

We are saving our data frame (tibble) to disk.

It is always a good idea to save results that required  many calculations (or took long time). 

If you one day want to work on the results - then you don't have to do all the calculations again. 

We save in the "rds" format with write_rds(x = object, file = filname)

```{r}

write_rds(snpdata, r"(U16\snpdata.rds)")
```

# Clean memory

Now we clean the memory and load the results again.

```{r}

# Look at your environment tab (top right in Rstudio)

# Then run this code
ls()

# What does ls() do?
# list environment variables

rm(list = ls())

# What did rm do?
# rm =remove
```


# Load results again from disc

```{r}

library(tidyverse)

snpdata <- read_rds(r"(U16\snpdata.rds)")

# snpdata
```


# Read the documentation for p.adjust()

?p.adjust

FROM docs:

Adjust P-values for Multiple Comparisons

Description
Given a set of p-values, returns p-values adjusted using one of several methods.

Usage

p.adjust(p, method = p.adjust.methods, n = length(p))

p.adjust.methods

# c("holm", "hochberg", "hommel", "bonferroni", "BH", "BY",
#   "fdr", "none")

Arguments

"p" numeric vector of p-values (possibly with NAs).

"method" correction method, a character string. Can be abbreviated.

## Example of use 

```{r}

results <- tibble(pvalue = c(
    0.0001, 0.001, 0.01, 0.05, 0.1,
    runif(n = 100, min = 0, max = 1)
))

results %>% mutate(adjusted_pvalue = p.adjust(p = pvalue, method = "bonferroni"))

```

>Q1: Adjust the original p values using the bonferroni adjustment

Add the results as a column to snpdata called "bonferroni"
```{r}
snpdata <- snpdata %>% mutate(bonferroni = p.adjust(p = p.value, method = "bonferroni"))
```

---

>Q2: Adjust the original p values using the false discovery rate (fdr)

Add the results as a column to snpdata called "fdr"
```{r}
snpdata <- snpdata %>% mutate(fdr = p.adjust(p = p.value, method = "fdr"))
```

---

# Working with adjusted p values (multiple testing)

This is just to show you different ways of counting.

```{r}

df <- tibble(x = c(0.01, 0.01, 0.5, 0.8, 0.9))

df %>%
    count(x <= 0.1)

df %>%
    mutate(testresult = (x <= 0.1)) %>%
    count(testresult)

df %>%
    summarise(
        smaller_than_0.1 = sum(x < 0.1),
        n = n(),
        proportion = smaller_than_0.1 / n
    )
```

>Q3: How many SNPs are significant using uncorrected p values (alpha = 0.05) ?
```{r}
snpdata %>% count(p.value <= 0.05)
```

---

>Q4: How many SNPs are significant using bonferroni corrected p values (alpha = 0.05) ?
```{r}
snpdata %>% count(bonferroni <= 0.05)
```


---

>Q5: How many SNPs are significant if we accept a false discovery rate of 10%?

I.e. then we expect ~10 % of all significant tests to be false positives.

```{r}
snpdata %>% count(fdr <= 0.1)
```


---

>Q6: How many SNPs are significant if we use accept a false discovery rate of 0.2?

```{r}
snpdata %>% count(fdr <= 0.2)
```

---

# The distribution of the significant SNPs in the genome

For the rest of the exercise we will focus on defining significant SNPs as:
* SNPs with an FDR <= 0.2 (false discovery rate)
All other SNPs are considered to be "not significant".

>Q7: How many SNPs are there in total on each chromosome? (all SNPs)

```{r}
snpdata %>%
    group_by(chromosome) %>%
    count()
```

---

>Q8: How many significant SNPs are there on each chromosome?
```{r}
snpdata %>%
    group_by(chromosome) %>%
    summarise(sig = sum(fdr <= 0.2))
```

---

>Q9: What is the proportion of significant SNPs on each chromosome?

So you need both the number of SNPs on each chromosome AND the number of "significant" SNPs on each chromosome
```{r}
snpdata %>%
    group_by(chromosome) %>%
    summarise(sigpercent = sum(fdr <= 0.2) / n())
```

---

>Q10: Make a plot of this proportion on each chromosome


---

>Q11: Which chromosome is most enriched for significant SNPs?


---

# Power analysis

Now, I want to plan my own study. I think I will test some cases and controls and count alleles in the two groups. (GWAS)

So I will have a 2x2 table (genotype X phenotype)

I assume that for SNPs that are involved in the disease the allele frequency in controls will be 0.2 (common allele) and that the disease allele frequency will be 0.25 (risk increased).

What is my power to detect disease associated alleles in these two cases?

I think I will use fisher.test()...

So your job:

1. Simulate 1000 datasets with random sampling under the ALTERNATIVE hypothesis
2. Run fisher test on each dataset
3. Count the number of false negatives
4. Calculate the power

You should do this for two scenarios:

>Scenario 1: 50 cases and 50 controls (a total of 200 alleles in 100 persons)

Hint:

```{r}

set.seed(1)

(x1 <- tibble(
    group = "cases",
    allele = sample(c("Ref", "Alt"), size = 200, prob = c(0.25, 0.75), replace = T)
))
(x2 <- tibble(
    group = "controls",
    allele = sample(c("Ref", "Alt"), size = 200, prob = c(0.20, 0.80), replace = T)
))

(x3 <- rbind(x1, x2))

table(x3$allele, x3$group)
(x <- fisher.test(x = x3$allele, y = x3$group))
x$p.value
```

You could simulate in other ways. This is just one way of getting 200 "Ref" or "Alt" alleles with given proportions and random sampling.

The sample() function has one drawback: the higher "n" the more time it uses. (There are faster ways to simulate 2x2 tables in R - but they are more difficult to understand). 

1
---

>Scenario 2: 100 cases and 100 controls (a total of 400 alleles in 200 persons)


---

# The daring bonus question

```{r}

x3 <- tibble(
    cases = rmultinom(n = 1, size = 1000000000, prob = c(0.25, 0.75)),
    controls = rmultinom(n = 1, size = 1000000000, prob = c(0.20, 0.80))
)

x3
```

>Q12: What do you think will happen if i run fisher.test(x3)?

WARNING: Don't run - it will freeze your computer.

# WARNING - SAVE YOUR WORK BEFORE YOU TRY IT!!!

If you are going to try it anyway.... go ahead.

```{r}
# fisher.test(x3)
```

